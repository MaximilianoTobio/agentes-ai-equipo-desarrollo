# üñ•Ô∏è Infraestructura y Despliegue -- Visi√≥n Arquitect√≥nica

> **Ubicaci√≥n sugerida:** `/docs/v2/04_infraestructura_y_despliegue.md`\
> **Prop√≥sito:** Describir la capa de infraestructura que soporta el
> sistema, sus garant√≠as (alta disponibilidad, aislamiento,
> persistencia, trazabilidad) y las decisiones t√©cnicas que definen c√≥mo
> se ejecuta el MVP v2 en producci√≥n.\
> **Nota:** Este documento NO es una gu√≠a paso a paso de instalaci√≥n. Es
> el porqu√© y el c√≥mo conceptual de la plataforma.

------------------------------------------------------------------------

## 1. Objetivo de la Infraestructura

La infraestructura est√° dise√±ada para sostener un sistema multi-agente
que:

-   Ejecute m√∫ltiples servicios en paralelo (orquestador + agentes +
    monitoreo).
-   Escale horizontalmente bajo carga (m√°s r√©plicas, no m√°s CPU en una
    sola instancia).
-   Se recupere de fallos sin intervenci√≥n manual (self-healing).
-   Mantenga trazabilidad total (qu√© pas√≥, cu√°ndo, y por qu√©).
-   Garantice que el entorno de ejecuci√≥n del c√≥digo generado por IA
    est√© aislado y auditado.

En otras palabras: **no es una "app web", es una f√°brica de software
aut√≥noma supervisada.**

------------------------------------------------------------------------

## 2. Topolog√≠a L√≥gica de la Plataforma

A nivel l√≥gico, tenemos cuatro dominios de infraestructura:

1.  **Dominio de Ejecuci√≥n de Servicios**
    -   Orchestrator (FastAPI)
    -   Agentes (DevAgent, QAAgent, ReviewAgent, RefactorAgent)
    -   PairCoordinator (subcomponente l√≥gico del orquestador)
2.  **Dominio de Datos y Mensajer√≠a**
    -   Postgres (estado del sistema, auditor√≠a, m√©tricas de performance
        XP)
    -   Redis Streams (cola de trabajo, coordinaci√≥n entre agentes)
    -   MinIO (artefactos de trabajo, snapshots, adjuntos de PR, etc.)
3.  **Dominio de Observabilidad**
    -   Prometheus (m√©tricas)
    -   Grafana (dashboards)
    -   Jaeger (distributed tracing)
4.  **Dominio de Entrada / Exposici√≥n**
    -   Traefik (reverse proxy + TLS)
    -   GitHub Webhooks (evento de entrada que convierte "intenci√≥n" en
        "tarea")
    -   Portainer / panel operativo interno (gesti√≥n de servicios)

Cada dominio puede evolucionar de forma independiente, pero todos deben
mantenerse coordinados por el orquestador.

------------------------------------------------------------------------

## 3. Plataforma de Ejecuci√≥n

La plataforma objetivo es un entorno basado en **Docker Swarm** en un
servidor controlado (por ejemplo, Contabo).\
Razones arquitect√≥nicas para usar Swarm en este MVP:

-   **Simplicidad operativa:** una sola m√°quina f√≠sica puede comportarse
    "como un cluster", con redes l√≥gicas, balanceo interno e incluso
    auto-restart de servicios.
-   **Declaratividad:** los servicios se describen como una pila (stack)
    y Swarm los mantiene vivos seg√∫n esa definici√≥n.
-   **Escalado horizontal por servicio:** `replicas: 3` en el
    orquestador es suficiente para alta disponibilidad dentro del mismo
    host (o entre hosts si se ampl√≠a).
-   **Compatibilidad con Traefik y Portainer:** monitoreo visual y
    certificados TLS automatizados.

> Decisi√≥n: no se usa Kubernetes en esta etapa porque agrega complejidad
> operativa y cognitiva que no es necesaria para el alcance actual del
> MVP.

------------------------------------------------------------------------

## 4. Redes Internas y Superficies Expuestas

### 4.1 Redes L√≥gicas

La infraestructura define al menos dos redes overlay dentro del cluster
Swarm:

-   `labstack_net`\
    Uso: comunicaci√≥n interna entre servicios cr√≠ticos del sistema
    (orchestrator, agentes, redis, postgres, minio).\
    Propiedad: tr√°fico de aplicaci√≥n.

-   `labstack_monitoring`\
    Uso: Prometheus, Grafana, Jaeger y futuros servicios de
    auditor√≠a/alerting.\
    Propiedad: telemetr√≠a y observabilidad.

Separar las redes permite: - Limitar qu√© servicios quedan accesibles
entre s√≠ (aislamiento por funci√≥n). - Reducir superficie de ataque (no
todo puede hablar con todo). - Controlar mejor qu√© termina expuesto v√≠a
Traefik.

### 4.2 Exposici√≥n Externa

Traefik es el gateway HTTP(S) hacia el exterior:

-   Termina TLS (certificados).
-   Hace routing basado en hostname (`orchestrator.labstation.dev`,
    `grafana.labstation.dev`, etc.).
-   Balancea carga entre m√∫ltiples r√©plicas del orquestador.

Ning√∫n agente se expone p√∫blicamente.\
Solo el orquestador, paneles de observabilidad internos (opcional) y
Portainer pueden tener rutas p√∫blicas (idealmente con auth/whitelist).

**Principio:** "Los agentes no hablan con el mundo. El mundo habla con
el orquestador."

------------------------------------------------------------------------

## 5. Persistencia y Estado

### 5.1 Postgres (verdad can√≥nica)

Postgres es el **source of truth** del sistema. Aqu√≠ se guardan:

-   Tareas
-   Estado de las sesiones de pair programming
-   Historial de PRs
-   M√©tricas de ejecuci√≥n de agentes (tiempos, resultados, cobertura)
-   Audit trail inmutable (qui√©n hizo qu√© y cu√°ndo)

Se extiende con `pgvector` para almacenar embeddings o contexto
sem√°ntico usado por los agentes.

**Decisi√≥n arquitect√≥nica clave:**\
\> "Nada es 's√≥lo en memoria'. Todo estado importante debe terminar en
Postgres."

### 5.2 Redis Streams (flujo operativo)

Redis no es la fuente de verdad. Redis es el **bus de trabajo**.

-   Los agentes consumen tareas desde streams (`XREADGROUP`).
-   Cada mensaje requiere ACK.
-   Los mensajes pendientes pueden ser reclamados si un agente muere
    (evita p√©rdida silenciosa de trabajo).
-   El orquestador orquesta la reasignaci√≥n.

Esto da: - Paralelismo seguro (muchos workers). - Garant√≠a de entrega al
menos una vez. - Capacidad de backpressure (si la cola crece, escalamos
workers).

### 5.3 MinIO (artefactos f√≠sicos)

MinIO act√∫a como almacenamiento S3-compat para: - Snapshots de sesiones
de pair programming - Archivos generados por los agentes (c√≥digo
propuesto, diffs) - Adjuntos de validaciones QA (reportes, coverage,
etc.)

Esto hace posible auditar una entrega incluso meses despu√©s, a√∫n si el
repo cambi√≥.

### 5.4 Respaldo y Retenci√≥n

-   Postgres ‚Üí backups programados, retenidos fuera del disco principal.
-   MinIO ‚Üí retenci√≥n versionada de artefactos cr√≠ticos.
-   Redis ‚Üí persistencia configurada (`appendonly=yes`) para recuperar
    colas tras reinicio.

> Decisi√≥n cr√≠tica: "Perder Redis 1 minuto es tolerable si Postgres
> tiene el estado. Perder Postgres es inaceptable."

------------------------------------------------------------------------

## 6. Alta Disponibilidad y Recuperaci√≥n

### 6.1 Orchestrator en Alta Disponibilidad

El orquestador corre en m√∫ltiples r√©plicas (ej. 3).\
Beneficios:

-   Si una r√©plica cae, Traefik reenv√≠a requests a otra.
-   El healthcheck HTTP (`/health`) y readiness `/ready` definen si una
    r√©plica puede recibir tr√°fico.
-   Evita que el orquestador sea un √∫nico punto de falla (SPOF).

### 6.2 Agentes como Workers Escalables

Los agentes (DevAgent, QAAgent, etc.) son procesos consumiendo streams.

-   Cada agente puede tener N r√©plicas simult√°neas.
-   Si sube la cola (`XLEN` crece), se pueden escalar r√©plicas de ese
    agente.
-   Si una r√©plica muere a mitad de trabajo, Redis marca mensajes como
    "pendientes sin ACK" ‚Üí otro worker puede retomarlos.

Esto soporta picos de trabajo sin redise√±ar nada.

### 6.3 Degradaci√≥n Controlada

Si falla un subsistema: - Falla Redis ‚Üí deja de haber trabajo nuevo,
pero no se corrompe el estado en Postgres. - Falla MinIO ‚Üí se pausa el
almacenamiento de artefactos, pero los agentes pueden seguir generando y
la auditor√≠a registra el intento fallido. - Falla Postgres ‚Üí el sistema
entra en modo "no aceptar nuevas tareas", porque no se puede registrar
auditablemente nada.

Esta priorizaci√≥n protege la trazabilidad: si no podemos registrar lo
que pasa, preferimos no avanzar.

------------------------------------------------------------------------

## 7. Observabilidad y Auditor√≠a

### 7.1 M√©tricas T√©cnicas

Prometheus recolecta m√©tricas de: - Salud del orquestador - Latencia de
endpoints - Tama√±o de la cola en Redis - Tasa de fallos de QA - Uso de
tokens por agente / sprint

Estas m√©tricas alimentan dashboards en Grafana y sirven para: - Ver
cuello de botella inmediato (cola creciendo ‚Üí necesitamos m√°s
DevAgent) - Ver degradaci√≥n de calidad (fallos de QA en aumento) - Ver
patrones de costo (excesivo consumo de tokens en tareas simples)

### 7.2 Trazas Distribuidas

Jaeger + OpenTelemetry rastrean una tarea extremo a extremo: - Webhook
recibido - Asignaci√≥n a DevAgent - QAAgent ejecutando validaciones -
ReviewAgent decidiendo merge

Beneficio: cuando algo sale mal, se puede ver en qu√© punto espec√≠fico se
rompi√≥ la cadena.

### 7.3 Auditor√≠a Operativa

El sistema escribe un audit trail inmutable en Postgres, con particiones
mensuales.\
Esto nos permite contestar preguntas como:

-   ¬øQui√©n (qu√© agente) aprob√≥ este merge?\
-   ¬øCon qu√© cobertura de tests?\
-   ¬øCu√°l era el estado de la cola en ese momento?\
-   ¬øCon qu√© prompt gener√≥ el c√≥digo?

Este punto es esencial para compliance interno, debugging y
post-mortems.

------------------------------------------------------------------------

## 8. Seguridad y Aislamiento en Ejecuci√≥n

### 8.1 Aislamiento de Ejecuci√≥n de C√≥digo

El c√≥digo propuesto por DevAgent (o revisado en QAAgent) **no se ejecuta
directamente en el host**.\
Se ejecuta dentro de un entorno aislado ("sandbox seguro") con:

-   Contenedor dedicado
-   L√≠mite de CPU y memoria
-   Red restringida o totalmente cerrada
-   Sin acceso a secretos de producci√≥n

Raz√≥n: el agente est√° generando c√≥digo din√°micamente. Ese c√≥digo debe
tratarse como no confiable.

### 8.2 Gesti√≥n de Secrets

Las credenciales sensibles (API keys, tokens de GitHub, claves JWT) no
viajan entre agentes en claro. - Viven como variables de entorno
inyectadas en cada servicio en desplegue. - No se exponen dentro de los
artefactos en MinIO. - No se guardan en logs ni en audit trail.

### 8.3 Control de Acceso

Traefik + reglas de enrutamiento + autenticaci√≥n en endpoints de
administraci√≥n limitan qui√©n puede: - Ver dashboards de Grafana -
Iniciar un despliegue seguro - Forzar un rollback

------------------------------------------------------------------------

## 9. Filosof√≠a Operativa

1.  **Nada es manual, todo es reproducible.**
    -   Si hay que hacer algo manual y urgente (ej. rollback), debe
        quedar registrado y tener un script equivalente documentado.
2.  **El sistema debe poder contarse a s√≠ mismo.**
    -   A trav√©s de m√©tricas, trazas y audit trails, el sistema puede
        explicar qu√© hizo, por qu√© lo hizo y con qu√© confiabilidad.
3.  **Los agentes son trabajadores reemplazables, no fuentes de
    verdad.**
    -   Podemos matar y recrear agentes libremente.\
    -   No pueden poseer estado √∫nico en memoria.\
    -   El estado importante vive en Postgres y MinIO.
4.  **El orquestador es el juez, no el ejecutor.**
    -   Orquesta, valida, bloquea, libera, audita.\
    -   No es quien genera el c√≥digo final ni quien lo prueba.\
    -   Esto mantiene responsabilidades claras y auditables.

------------------------------------------------------------------------

## 10. Estado Actual y Evoluci√≥n Esperada

-   La plataforma actual soporta un MVP robusto (v2) en un √∫nico nodo
    Swarm con alta disponibilidad l√≥gica.
-   Puede ampliarse a m√∫ltiples nodos sin redise√±ar conceptos clave
    (r√©plicas, redes overlay, Traefik, Redis Streams).
-   A futuro (Sprint 3+), se eval√∫a:
    -   Canary releases por servicio
    -   Auto-escalado reactivo basado en tama√±o de cola y latencia QA
    -   Feature flags para versiones de agentes
    -   Separaci√≥n f√≠sica de entornos (staging vs producci√≥n)

------------------------------------------------------------------------

**Versi√≥n:** v2.0 --- Octubre 2025\
**Autor:** Equipo de Ingenier√≠a -- Proyecto Agentes AI
